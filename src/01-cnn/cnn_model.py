# -*- coding: utf-8 -*-
"""cnn_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkQdGMKp_x63dXqwU5H1bX8gckv6Mjjt
"""

from tqdm import tqdm
import numpy as np
import pandas as pd

import tensorflow as tf
from keras import Sequential
from keras.layers import Flatten, Dense
from keras.layers import Input
from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

"""## Read combined text dataset"""

df = pd.read_csv('drive/Team Drives/Deep Learning Project/ken_cnn/combined_text_dataset.csv', sep='\t')

if 'Unnamed: 0' in df.columns:
  df.drop('Unnamed: 0', axis=1, inplace=True) # Drop unused column
df.head(3)

# Show stats of dataset
home_count = df.groupby('winner')['winner'].count()['home']
away_count = df.groupby('winner')['winner'].count()['away']
tie_count = df.groupby('winner')['winner'].count()['tie']

print("Home wins: {} ==> {}%".format(home_count, home_count * 100 / df.shape[0]))
print("Away wins: {} ==> {}%".format(away_count, away_count * 100 / df.shape[0]))
print("Tie: {} ==> {}%".format(tie_count, tie_count * 100 / df.shape[0]))

"""## One-hot Encoding"""

# Check number of unique words
unique_words = set()
max_len = -1
for idx, row in tqdm(df.iterrows(), total=df.shape[0]):
  split_text = row['text'].split()
  for t in split_text:
    unique_words.add(t)
    
  # Check max length
  if len(split_text) > max_len:
    max_len = len(split_text)

print("\nTotal number of unique words: {0}".format(len(unique_words)))
print("Max comment word length: {0}".format(max_len))

# num_words = len(unique_words)
num_words = 200 # Most 1000 common words

import pickle
from keras.preprocessing.text import Tokenizer
samples = list(df.text)
# Creates a tokenizer, configured to only take into account the <num_words> most common words
tokenizer = Tokenizer(num_words=num_words)
# Building the word index
tokenizer.fit_on_texts(samples)

# Turns strings into lists of integer indices
sequences = tokenizer.texts_to_sequences(samples)
# sequences[0]

# Turns string into binary vector of dim 1000 (based on word limit above)
# one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')
# pd.DataFrame(one_hot_results).head(3)

# Dictionary mapping of words to one-hot-encoded index value
word_index = tokenizer.word_index
print('Found {} unique tokens'.format(len(word_index)))
print('The dictionary mapping of tokens is\n {}'.format(word_index))

"""## Feature Preparation"""

# --- Setting up constants ---
# Number of words as features, we keep only the top most-common words
max_features = 1000
# Max number of words in comments (truncate the rest)
max_len = 1132 # If not set here, will use the max comment length which is 1132

from keras import preprocessing
x_train = preprocessing.sequence.pad_sequences(sequences, maxlen = max_len)
x_train.shape

# # --- Use one-hot encode ---
# one_hot_x_train = []
# for seq in tqdm(x_train, total=x_train.shape[0]):
#   feature_vector = []
#   for i in seq:
#     feature_vector += list(one_hot_results[i])
#   one_hot_x_train.append(feature_vector)

# one_hot_x_train = pd.DataFrame(one_hot_x_train)
# one_hot_x_train.shape

# one_hot_x_train[0]

def get_label(text):
  if text == "home":
    return [1,0,0]
  elif text == "away":
    return [0,0,1]
  elif text == 'tie':
    return [0,1,0]

y_train = np.array(list(df.winner.map(get_label)))
len(y_train)

"""## Simple ANN model"""

# Set parameters
# input_number = num_words * max_len # Length of one-hot vector

# # --- Setting up a Sigmoid Sequential Model ---
# # Initialize model
# model = Sequential()
# # Adds a densely-connected layer with input_number units to the model:
# model.add(Dense(32, input_shape = (input_number,), activation='sigmoid'))
# # Add another:
# model.add(Dense(64, activation='relu'))
# # Add another:
# model.add(Dense(32, activation='relu'))
# # Add a softmax layer with 3 output units:
# model.add(Dense(3, activation='softmax'))
# # Check model summary
# model.summary()

# model.compile(loss='categorical_crossentropy',
#               optimizer='adam',
#               metrics=['accuracy'])

# model.fit(one_hot_x_train, y_train,
#                    epochs = 2,
#                    batch_size = 64,
#                    validation_split = 0.2)

"""## Top team to be test"""

top_num = 1

import operator
from tqdm import trange

def get_top_win_teams(df):
  win_counts = dict()
  
  for idx in trange(df.shape[0]):
    ht = df.iloc[idx]['ht']
    at = df.iloc[idx]['at']
    winner = df.iloc[idx]['winner']
    
    if winner == 'home':
      winner_team = ht
    elif winner == 'away':
      winner_team = at
    else:
      continue
      
    if winner_team in win_counts:
      win_counts[winner_team] += 1
    else:
      win_counts[winner_team] = 1
      
  sorted_win_counts = sorted(win_counts.items(), key=operator.itemgetter(1), reverse=True)
    
  return [t for t, score in sorted_win_counts]

top_team_names = get_top_win_teams(df)
top_team_names = top_team_names[:top_num] # Top 10

def get_top_team_won(df, top_team_names):
  indexes = []
  for i in trange(df.shape[0]):
    ht = df.iloc[i]['ht']
    at = df.iloc[i]['at']
    if (ht in top_team_names) or (at in top_team_names):
      indexes.append(i)
    
  return df.iloc[indexes]

top_team_df = get_top_team_won(df, top_team_names)

test_sequences = tokenizer.texts_to_sequences(list(top_team_df.text))
x_test = preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_len)
x_test.shape

y_test = pd.DataFrame([ get_label(winner.strip()) for winner in top_team_df['winner'] ])
y_test.shape

"""## CNN model"""

# Set parameters
input_number = max_len # Length of input
vocabulary_size = len(unique_words)

# --- Setting up a Sigmoid Sequential Model ---
# Initialize model
model_conv = Sequential()
model_conv.add(Embedding(vocabulary_size, 256, input_length=input_number))
model_conv.add(Dropout(0.2))
model_conv.add(Conv1D(64, 5, activation='relu'))
model_conv.add(MaxPooling1D(pool_size=4))
model_conv.add(Conv1D(32, 5, activation='relu'))
model_conv.add(MaxPooling1D(pool_size=4))
model_conv.add(Conv1D(16, 5, activation='relu'))
model_conv.add(MaxPooling1D(pool_size=4))
model_conv.add(Flatten())
model_conv.add(Dense(3, activation='softmax'))
model_conv.summary()

model_conv.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_conv.fit(x_train, y_train,
                   epochs = 7,
                   batch_size = 64,
                   validation_data=(x_test, y_test))

# validation_split = 0.2, it gives val_acc=0.7085 and val_loss=0.5091

model_fp = 'drive/Team Drives/Deep Learning Project/ken_cnn/cnn_64_fixed.model'
# model_conv.save(model_fp)